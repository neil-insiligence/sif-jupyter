{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecae899b",
   "metadata": {},
   "source": [
    "# SIF workflow for characterising synthetic proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a110e",
   "metadata": {},
   "source": [
    "# This is the development version 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e8956",
   "metadata": {},
   "source": [
    "### First define the data sources - there should be a synthetic set of sequences as a single fasta file and single natural set of sequences as a fasta file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07143d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the dataset names\n",
    "synName = os.environ['SYNPROTS_NAME'] = 'ISP_1111_A1_short'\n",
    "natName = os.environ['NATPROTS_NAME'] = 'sdr_pdg_filtered_short'\n",
    "\n",
    "# Locations of the data folders\n",
    "\n",
    "# Synthetic protein input data lives here\n",
    "os.environ['SYNPROTS'] = '/home/neil/data/sif/synProts/ISP_1111_A1/'\n",
    "# Natural protein input data lives here\n",
    "os.environ['NATPROTS'] = '/home/neil/data/sif/natProts/zhen_sdrs/'\n",
    "\n",
    "# Access the environment variable later in code\n",
    "synProts = os.environ['SYNPROTS']\n",
    "natProts = os.environ['NATPROTS']\n",
    "\n",
    "# Synthetic protein structure data lives here\n",
    "synStrucs = os.environ['SYNSTRUCS'] = '/home/neil/data/sif/synStrucs/'\n",
    "\n",
    "# Natural protein structure data lives here \n",
    "natStrucs = os.environ['NATSTRUCS'] = '/home/neil/data/sif/natStrucs/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2697008e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting click (from nltk)\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2023.6.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.3 joblib-1.3.1 nltk-3.8.1 regex-2023.6.3 tqdm-4.65.0\n"
     ]
    }
   ],
   "source": [
    "#Natural language library\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d290b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/neil/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow ID is: conspiracy-chances-283886\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "import random\n",
    "import uuid\n",
    "from collections import Counter\n",
    "\n",
    "# Download the brown corpus\n",
    "nltk.download('brown')\n",
    "\n",
    "# Get a list of English words\n",
    "word_list = nltk.corpus.brown.words()\n",
    "\n",
    "# Count the occurrences of each word\n",
    "word_counter = Counter(word_list)\n",
    "\n",
    "# Get the 5000 most common words\n",
    "common_words = [word for word, _ in word_counter.most_common(5000)]\n",
    "\n",
    "def generate_human_memorable_id():\n",
    "    # Generate a unique ID using uuid\n",
    "    unique_id = uuid.uuid4().int\n",
    "\n",
    "    # Select two random words\n",
    "    word1 = random.choice(common_words)\n",
    "    word2 = random.choice(common_words)\n",
    "\n",
    "    # Truncate the UUID to the last 6 digits for brevity and append it to the words\n",
    "    human_memorable_id = f'{word1}-{word2}-{str(unique_id)[-6:]}'\n",
    "\n",
    "    return human_memorable_id\n",
    "\n",
    "# Print a unique, human-memorable ID\n",
    "wf_id = generate_human_memorable_id()\n",
    "print(\"Workflow ID is:\", wf_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1bfbd7",
   "metadata": {},
   "source": [
    "### Structural Predictions using esmFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f7dff",
   "metadata": {},
   "source": [
    "**Fold the synthetic proteins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f61a1668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py3Dmol\n",
      "  Using cached py3Dmol-2.0.3-py2.py3-none-any.whl (12 kB)\n",
      "Installing collected packages: py3Dmol\n",
      "Successfully installed py3Dmol-2.0.3\n",
      "Collecting biopython\n",
      "  Downloading biopython-1.81-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy (from biopython)\n",
      "  Using cached numpy-1.25.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "Installing collected packages: numpy, biopython\n",
      "Successfully installed biopython-1.81 numpy-1.25.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/neil/projects-dep/esmFold/esmFold.py\", line 3, in <module>\n",
      "    from transformers.utils import send_example_telemetry\n",
      "ModuleNotFoundError: No module named 'transformers'\n",
      "/home/neil/projects-dep/esmFold/esmFold.py /home/neil/data/sif/synProts/ISP_1111_A1/ISP_1111_A1_short.fasta /home/neil/data/sif/synStrucs/conspiracy-chances-283886/ISP_1111_A1_short\n"
     ]
    }
   ],
   "source": [
    "!pip install py3Dmol\n",
    "!pip install biopython\n",
    "\n",
    "# Build the complete paths first\n",
    "fasta_path = f\"{synProts}{synName}.fasta\"\n",
    "output_path = f\"{synStrucs}{wf_id}/{synName}\"\n",
    "\n",
    "# Use the paths in the command\n",
    "!python /home/neil/projects-dep/esmFold/esmFold.py {fasta_path} {output_path}\n",
    "print(\"/home/neil/projects-dep/esmFold/esmFold.py\", fasta_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f534a0",
   "metadata": {},
   "source": [
    "**Fold the natural proteins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f03ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the complete paths first\n",
    "fasta_path = f\"{natProts}{natName}.fasta\"\n",
    "output_path = f\"{natStrucs}{wf_id}/{natName}\"\n",
    "\n",
    "# Use the paths in the command\n",
    "!python /home/neil/projects-dep/esmFold/esmFold.py {fasta_path} {output_path}\n",
    "print(\"/home/neil/projects-dep/esmFold/esmFold.py\", fasta_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c088731",
   "metadata": {},
   "source": [
    "#Building the SSN. We are going to do this manually over a terminal to TBone as it can take a while.\n",
    "\n",
    "1) SSH into TBone e.g. ssh neil@92.40.34.250\n",
    "2) conda activate ssn\n",
    "3) run the command generated below so that things end up in the right directory\n",
    "4) export NXF_VER=22.10.0\n",
    "5) nextflow run ravenlocke/nf-needleall-ava --infile /home/neil/data/sif/synProts/ISP_1111_A1/ISP_1111_A1_short.fasta --outdir /home/neil/data/sif/synProtComps/ssn/ --threshold 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41748a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print (\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
